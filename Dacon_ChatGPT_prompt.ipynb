{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1M8pDzgEvNnTiQIlboBzl6ly2pGL2xLne",
      "authorship_tag": "ABX9TyM8BbL3upzTA+xGdO+cZz6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbtjr1103/Competition_practice/blob/main/Dacon_ChatGPT_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# train.csv 파일 불러오기\n",
        "train = pd.read_csv('/content/drive/MyDrive/ChatGPT/train.csv')\n",
        "\n",
        "# test.csv 파일 불러오기\n",
        "test = pd.read_csv('/content/drive/MyDrive/ChatGPT/test.csv')\n",
        "\n",
        "# sample_submission.csv 파일 불러오기\n",
        "submission = pd.read_csv('/content/drive/MyDrive/ChatGPT/sample_submission.csv')\n"
      ],
      "metadata": {
        "id": "_IgmBD0rGOYX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 파일의 첫 5행 출력하기\n",
        "print(train.head())\n",
        "\n",
        "# train 파일의 크기 출력하기\n",
        "print(train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC_kpGASGdQd",
        "outputId": "6ea7c62a-bab7-43b6-9369-a1a67a251a0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            id                                               text  label\n",
            "0  TRAIN_00000  Israel Parliament to Start Winter Session JERU...      3\n",
            "1  TRAIN_00001  Two-thirds of business owners say they are pre...      2\n",
            "2  TRAIN_00002  Story highlightsRed Bull team principal Christ...      1\n",
            "3  TRAIN_00003  Final respects paid to Arafat Palestinians pay...      3\n",
            "4  TRAIN_00004  Steelers winning the old fashioned way -- they...      1\n",
            "(47399, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# train 데이터에서 첫번째 문장을 예시로 사용\n",
        "text = train['text'][0]\n",
        "\n",
        "# 문장을 단어로 분리하여 리스트로 저장\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 출력\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO0WDAIwGdIk",
        "outputId": "f70da564-9bbd-4232-abee-f1fc994f9603"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Israel', 'Parliament', 'to', 'Start', 'Winter', 'Session', 'JERUSALEM', '-', 'Prime', 'Minister', 'Ariel', 'Sharon', ',', 'who', 'has', 'alienated', 'his', 'right-wing', 'Likud', 'party', 'with', 'plans', 'to', 'withdraw', 'from', 'Gaza', ',', 'begins', 'a', 'new', 'legislative', 'session', 'fighting', 'for', 'his', 'government', \"'s\", 'survival', '.', 'Lawmakers', 'were', 'scheduled', 'to', 'vote', 'on', 'two', 'motions', 'of', 'no', 'confidence', 'Monday', ',', 'the', 'first', 'day', 'of', 'Israel', \"'s\", 'winter', 'parliamentary', 'session', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# train 데이터에서 첫번째 문장을 예시로 사용\n",
        "text = train['text'][0]\n",
        "\n",
        "# 문장을 단어로 분리하여 리스트로 저장\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 단어의 품사를 태깅하여 리스트로 저장\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# 출력\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDJhj9sXH_W8",
        "outputId": "2364b53e-15bd-4d77-dfc8-fc6cdf6d865f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Israel', 'NNP'), ('Parliament', 'NNP'), ('to', 'TO'), ('Start', 'NNP'), ('Winter', 'NNP'), ('Session', 'NNP'), ('JERUSALEM', 'NNP'), ('-', ':'), ('Prime', 'NNP'), ('Minister', 'NNP'), ('Ariel', 'NNP'), ('Sharon', 'NNP'), (',', ','), ('who', 'WP'), ('has', 'VBZ'), ('alienated', 'VBN'), ('his', 'PRP$'), ('right-wing', 'JJ'), ('Likud', 'NNP'), ('party', 'NN'), ('with', 'IN'), ('plans', 'NNS'), ('to', 'TO'), ('withdraw', 'VB'), ('from', 'IN'), ('Gaza', 'NNP'), (',', ','), ('begins', 'VBZ'), ('a', 'DT'), ('new', 'JJ'), ('legislative', 'JJ'), ('session', 'NN'), ('fighting', 'VBG'), ('for', 'IN'), ('his', 'PRP$'), ('government', 'NN'), (\"'s\", 'POS'), ('survival', 'NN'), ('.', '.'), ('Lawmakers', 'NNS'), ('were', 'VBD'), ('scheduled', 'VBN'), ('to', 'TO'), ('vote', 'VB'), ('on', 'IN'), ('two', 'CD'), ('motions', 'NNS'), ('of', 'IN'), ('no', 'DT'), ('confidence', 'NN'), ('Monday', 'NNP'), (',', ','), ('the', 'DT'), ('first', 'JJ'), ('day', 'NN'), ('of', 'IN'), ('Israel', 'NNP'), (\"'s\", 'POS'), ('winter', 'NN'), ('parliamentary', 'JJ'), ('session', 'NN'), ('...', ':')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('chunking')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# train 데이터에서 첫번째 문장을 예시로 사용\n",
        "text = train['text'][0]\n",
        "\n",
        "# 문장을 단어로 분리하여 리스트로 저장\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 단어의 품사를 태깅하여 리스트로 저장\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# 구문 분석을 수행하여 트리 구조로 저장\n",
        "tree = nltk.chunk.ne_chunk(tagged)\n",
        "\n",
        "# 출력\n",
        "print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOKrnOHrIJKD",
        "outputId": "761c3a07-5581-4348-bc3e-81b04270cf34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Israel/NNP)\n",
            "  (ORGANIZATION Parliament/NNP)\n",
            "  to/TO\n",
            "  (PERSON Start/NNP Winter/NNP Session/NNP JERUSALEM/NNP)\n",
            "  -/:\n",
            "  Prime/NNP\n",
            "  Minister/NNP\n",
            "  (PERSON Ariel/NNP Sharon/NNP)\n",
            "  ,/,\n",
            "  who/WP\n",
            "  has/VBZ\n",
            "  alienated/VBN\n",
            "  his/PRP$\n",
            "  right-wing/JJ\n",
            "  (ORGANIZATION Likud/NNP)\n",
            "  party/NN\n",
            "  with/IN\n",
            "  plans/NNS\n",
            "  to/TO\n",
            "  withdraw/VB\n",
            "  from/IN\n",
            "  (GPE Gaza/NNP)\n",
            "  ,/,\n",
            "  begins/VBZ\n",
            "  a/DT\n",
            "  new/JJ\n",
            "  legislative/JJ\n",
            "  session/NN\n",
            "  fighting/VBG\n",
            "  for/IN\n",
            "  his/PRP$\n",
            "  government/NN\n",
            "  's/POS\n",
            "  survival/NN\n",
            "  ./.\n",
            "  Lawmakers/NNS\n",
            "  were/VBD\n",
            "  scheduled/VBN\n",
            "  to/TO\n",
            "  vote/VB\n",
            "  on/IN\n",
            "  two/CD\n",
            "  motions/NNS\n",
            "  of/IN\n",
            "  no/DT\n",
            "  confidence/NN\n",
            "  Monday/NNP\n",
            "  ,/,\n",
            "  the/DT\n",
            "  first/JJ\n",
            "  day/NN\n",
            "  of/IN\n",
            "  (GPE Israel/NNP)\n",
            "  's/POS\n",
            "  winter/NN\n",
            "  parliamentary/JJ\n",
            "  session/NN\n",
            "  .../:)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Error loading chunking: Package 'chunking' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# train 데이터에서 첫번째 문장을 예시로 사용\n",
        "text = train['text'][0]\n",
        "\n",
        "# 문장을 단어로 분리하여 리스트로 저장\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 단어의 품사를 태깅하여 리스트로 저장\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# WordNetLemmatizer 객체를 생성하여 단어를 표준형으로 변환\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = []\n",
        "for token, tag in tagged:\n",
        "    pos = get_wordnet_pos(tag)\n",
        "    if pos:\n",
        "        lemma = lemmatizer.lemmatize(token, pos)\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(token)\n",
        "    lemmatized.append(lemma)\n",
        "\n",
        "# 출력\n",
        "print(lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfSi1BfZITYE",
        "outputId": "30d827b5-6f58-4f31-8c3d-8b8ea3d1bdfe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Israel', 'Parliament', 'to', 'Start', 'Winter', 'Session', 'JERUSALEM', '-', 'Prime', 'Minister', 'Ariel', 'Sharon', ',', 'who', 'have', 'alienate', 'his', 'right-wing', 'Likud', 'party', 'with', 'plan', 'to', 'withdraw', 'from', 'Gaza', ',', 'begin', 'a', 'new', 'legislative', 'session', 'fight', 'for', 'his', 'government', \"'s\", 'survival', '.', 'Lawmakers', 'be', 'schedule', 'to', 'vote', 'on', 'two', 'motion', 'of', 'no', 'confidence', 'Monday', ',', 'the', 'first', 'day', 'of', 'Israel', \"'s\", 'winter', 'parliamentary', 'session', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
        "    tag = tag[0].upper()\n",
        "    if tag == 'J':\n",
        "        return wordnet.ADJ\n",
        "    elif tag == 'N':\n",
        "        return wordnet.NOUN\n",
        "    elif tag == 'V':\n",
        "        return wordnet.VERB\n",
        "    elif tag == 'R':\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "LviiSa9EI7oc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def preprocess(text):\n",
        "    # 소문자 변환\n",
        "    text = text.lower()\n",
        "    # 특수문자 제거\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # 토큰화\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # 불용어 제거\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if not token in stop_words]\n",
        "    # 형태소 분석\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    lemmatized = []\n",
        "    for token, tag in tagged:\n",
        "        pos = get_wordnet_pos(tag)\n",
        "        if pos:\n",
        "            lemma = lemmatizer.lemmatize(token, pos)\n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "        lemmatized.append(lemma)\n",
        "    # 구문 분석\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    tree = cp.parse(nltk.pos_tag(lemmatized))\n",
        "    # 의미 분석\n",
        "    synsets = []\n",
        "    for word in lemmatized:\n",
        "        for synset in wordnet.synsets(word):\n",
        "            if synset not in synsets:\n",
        "                synsets.append(synset)\n",
        "    return lemmatized, tree, synsets\n"
      ],
      "metadata": {
        "id": "iT5icdPzJQYF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def get_tfidf_vectors(corpus):\n",
        "    vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    return X.toarray(), vectorizer.get_feature_names()\n",
        "\n",
        "def get_word2vec_vectors(corpus):\n",
        "    # TODO: Word2Vec vectorization\n",
        "    pass\n",
        "\n",
        "def get_vectors(corpus, method='tf-idf'):\n",
        "    if method == 'tf-idf':\n",
        "        return get_tfidf_vectors(corpus)\n",
        "    elif method == 'word2vec':\n",
        "        return get_word2vec_vectors(corpus)\n",
        "    else:\n",
        "        raise ValueError('Invalid vectorization method')\n"
      ],
      "metadata": {
        "id": "lg-RjwNrKMMM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_model(X_train, y_train, method='svm'):\n",
        "    if method == 'svm':\n",
        "        clf = SVC(kernel='linear')\n",
        "    elif method == 'naive-bayes':\n",
        "        clf = MultinomialNB()\n",
        "    elif method == 'decision-tree':\n",
        "        clf = DecisionTreeClassifier()\n",
        "    elif method == 'random-forest':\n",
        "        clf = RandomForestClassifier()\n",
        "    else:\n",
        "        raise ValueError('Invalid classification method')\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n"
      ],
      "metadata": {
        "id": "n4UPVH4wKPXl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "wFH4MLbyKU3-",
        "outputId": "8c09c465-08b5-4674-fb54-a5858a8fe5e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c18baa0eb099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# train 데이터 불러오기\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/ChatGPT/train.csv')\n",
        "\n",
        "# 토큰화 함수\n",
        "def tokenize(text):\n",
        "    # 단어 토큰화\n",
        "    tokens = word_tokenize(text)\n",
        "    # 소문자 변환\n",
        "    lowercase_tokens = [token.lower() for token in tokens]\n",
        "    # 불용어 제거\n",
        "    filtered_tokens = [token for token in lowercase_tokens if token not in stopwords.words('english')]\n",
        "    return filtered_tokens\n",
        "\n",
        "# 형태소 분석 함수\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemmatize_words(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = []\n",
        "    for token, tag in pos_tag(tokens):\n",
        "        pos = get_wordnet_pos(tag)\n",
        "        if pos:\n",
        "            lemma = lemmatizer.lemmatize(token, pos)\n",
        "        else:\n",
        "            lemma = token\n",
        "        lemmatized.append(lemma)\n",
        "    return lemmatized\n",
        "\n",
        "# 구문 분석 함수\n",
        "def parse_sentence(text):\n",
        "    tagged = pos_tag(word_tokenize(text))\n",
        "    tree = ne_chunk(tagged)\n",
        "    return tree\n",
        "\n",
        "# 의미 분석 함수\n",
        "def get_word_definition(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    if synsets:\n",
        "        definition = synsets[0].definition()\n",
        "    else:\n",
        "        definition = \"\"\n",
        "    return definition\n",
        "\n",
        "def create_train_data(data):\n",
        "    # 토큰화\n",
        "    data['tokens'] = data['text'].apply(tokenize)\n",
        "    # 형태소 분석\n",
        "    data['lemmatized'] = data['tokens'].apply(lemmatize_words)\n",
        "    # 구문 분석\n",
        "    data['parsed'] = data['text'].apply(parse_sentence)\n",
        "    # 의미 분석\n",
        "    data['definition'] = data['lemmatized'].apply(lambda x: [get_word_definition(word) for word in x])\n",
        "    return data\n",
        "\n",
        "# train 데이터 전처리\n",
        "train_data = create_train_data(train_data)\n",
        "\n",
        "# feature 추출\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = tfidf_vectorizer.fit_transform(train_data['text'])\n",
        "y_train = train_data['category']\n",
        "\n",
        "# 모델 학습\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# 모델 평가\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/ChatGPT/test.csv')\n",
        "test_data = create_train_data(test_data)\n",
        "X_test = tfidf_vectorizer.transform(test_data['text'])\n",
        "\n",
        "# 예측\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 평가\n",
        "print('Accuracy:', accuracy_score(test_data['category'], y_pred))\n",
        "print('Precision:', precision_score(test_data['category'], y_pred, average='macro'))\n",
        "print('Recall:', recall_score(test_data['category'], y_pred, average='macro'))\n",
        "print('F1-score:', f1_score(test_data['category'], y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "KN0m7RGmK1GK",
        "outputId": "dacfe22e-0290-48a8-a758-69bf9b73da53"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1cabb7b26326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# train 데이터 전처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# feature 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-1cabb7b26326>\u001b[0m in \u001b[0;36mcreate_train_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# 구문 분석\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m# 의미 분석\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'definition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_word_definition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-1cabb7b26326>\u001b[0m in \u001b[0;36mparse_sentence\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;31m# higher than that cutoff first; otherwise, return None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cutoff_prob\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mpdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mprob_classify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mf_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mprob_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu_ZHnHbK5si",
        "outputId": "d76bc3ab-106c-46c3-86d4-206fdf54677f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}